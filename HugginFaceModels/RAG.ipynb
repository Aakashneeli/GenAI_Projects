{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv , find_dotenv\n",
    "import os\n",
    "dotenv_path = find_dotenv()\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "api_key = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "if api_key is None:\n",
    "    raise ValueError(\"HUGGINGFACEHUB_API_TOKEN not found in environment variables\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator \n",
    "from langchain.embeddings import HuggingFaceHubEmbeddings\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoModelForCausalLM\n",
    "from langchain_community.vectorstores import FAISS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the PDF\n",
    "loader  = PyPDFLoader(\"C:\\\\Users\\\\Admin\\\\Documents\\\\gen_ai_training\\\\pdfs\\\\rag.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "#splitting\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100, add_start_index=True)\n",
    "chunks = text_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks indexed: 276\n"
     ]
    }
   ],
   "source": [
    "#Model \n",
    "modelPath = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "#HuggingFace Embedding \n",
    "embeddings = HuggingFaceHubEmbeddings(model=modelPath)\n",
    "\n",
    "db = FAISS.from_documents(chunks, embedding=embeddings)  # Pass embeddings as parameter\n",
    "print(f\"Total chunks indexed: {db.index.ntotal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline for text generation\n",
    "generation_model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(generation_model_name)\n",
    "generation_model = AutoModelForCausalLM.from_pretrained(generation_model_name)\n",
    "\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=-1)  # device=0 for GPU, -1 for CPU\n",
    "\n",
    "def generate_response(query, db, top_k=4):\n",
    "    # Perform similarity search to retrieve relevant text chunks\n",
    "    chunks = db.similarity_search(query, k=top_k)\n",
    "\n",
    "    # Combine the content of the top-k chunks into a single text string\n",
    "    combined_text = \"\\n\".join([chunk.page_content for chunk in chunks])\n",
    "\n",
    "    # Prepare the prompt\n",
    "    prompt = f\"Based on the following content, answer the query and if it's not in the content then say 'I don't know': {query}\\n\\nContent:\\n{combined_text}\\n\\nAnswer:\"\n",
    "\n",
    "    # Generate response using Hugging Face model\n",
    "    response = generator(prompt, max_new_tokens=150, num_return_sequences=1)\n",
    "    \n",
    "    # Extract the response text\n",
    "    response_text = response[0][\"generated_text\"].strip()\n",
    "    \n",
    "    # Return response along with metadata\n",
    "    return response_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response to 'explain thoroughly the 3 types of RAG?': Based on the following content, answer the query and if it's not in the content then say 'I don't know': explain thoroughly the 3 types of RAG?\n",
      "\n",
      "Content:\n",
      "question, form a comprehensive prompt that empowers LLMs\n",
      "to generate a well-informed answer.\n",
      "The RAG research paradigm is continuously evolving, and\n",
      "we categorize it into three stages: Naive RAG, Advanced\n",
      "RAG, and Modular RAG, as showed in Figure 3. Despite\n",
      "RAG method are cost-effective and surpass the performance\n",
      "of the native LLM, they also exhibit several limitations.\n",
      "The development of Advanced RAG and Modular RAG is\n",
      "a response to these specific shortcomings in Naive RAG.\n",
      "A. Naive RAG\n",
      "a response to these specific shortcomings in Naive RAG.\n",
      "A. Naive RAG\n",
      "The Naive RAG research paradigm represents the earli-\n",
      "est methodology, which gained prominence shortly after the\n",
      "In the development of RAG technology, there is a clear\n",
      "trend towards different specialization directions, such as: 1)\n",
      "Customization - tailoring RAG to meet specific requirements.\n",
      "2) Simplification - making RAG easier to use to reduce the\n",
      "11https://github.com/weaviate/Verba\n",
      "12https://aws.amazon.com/cn/kendra/\n",
      "3\n",
      "Fig. 2. A representative instance of the RAG process applied to question answering. It mainly consists of 3 steps. 1) Indexing. Documents are split into chunks,\n",
      "encoded into vectors, and stored in a vector database. 2) Retrieval. Retrieve the Top k chunks most relevant to the question based on semantic similarity. 3)\n",
      "Generation. Input the original question and the retrieved chunks together into LLM to generate the final answer.\n",
      "widespread adoption of ChatGPT. The Naive RAG follows\n",
      "\n",
      "Answer:\n",
      "\n",
      "There are three basic methods; using and unusing\n",
      "\n",
      "the above syntax, which are not\n",
      "\n",
      "easily implemented in the native LLM.\n",
      "\n",
      "A. \"Simple RAG\" is in practice a response to the\n",
      "\n",
      "questioning type's type. That is, a standardization of \"Simple RAG\" for\n",
      "\n",
      "questioning.\n",
      "\n",
      "B. We have already demonstrated what may be\n",
      "\n",
      "necessary or a practical use for \"Smart RAG\" such as:\n",
      "\n",
      "Q - Real Time\n",
      "\n",
      "R - Simple RAG\n",
      "\n",
      "A. A Real Time\n",
      "\n",
      "RAG is a solution to the problem of \"Not to be missed\" or \"To not to be missed\"\n",
      "\n",
      "and for\n"
     ]
    }
   ],
   "source": [
    "query = \"explain thoroughly the 3 types of RAG?\"\n",
    "response = generate_response(query, db)\n",
    "print(f\"Response to '{query}': {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
