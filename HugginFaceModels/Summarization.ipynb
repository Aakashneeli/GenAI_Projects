{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_huggingface import ChatHuggingFace \n",
    "import glob \n",
    "from transformers import pipeline\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import T5Tokenizer , T5ForConditionalGeneration\n",
    "import fitz\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Model and tokenizer \n",
    "checkpoint = \"MBZUAI/LaMini-Flan-T5-248M\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(checkpoint)\n",
    "base_model = T5ForConditionalGeneration.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file loader and preprocessor\n",
    "def file_preprocessing(file_path):\n",
    "    doc = fitz.open(file_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "\n",
    "    # Split the text into manageable chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50, add_start_index=True)\n",
    "    texts = text_splitter.split_text(text)\n",
    "\n",
    "    final_text = \"\"\n",
    "    for text in texts:\n",
    "        final_text += text\n",
    "    return final_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large language models (LLMs) have revolutionized the field of natural language\n",
      "processing (NLP) by demonstrating unprecedented capabilities in understand-ing and generating human-like text. These models, such as OpenAI’s GPT-3\n",
      "and GPT-4, are built using deep learning techniques, specifically transformers,which allow them to process and analyze vast amounts of text data. By training\n",
      "on diverse datasets comprising billions of words, LLMs can generate coherentand contextually relevant responses, making them useful for a wide range of ap-\n",
      "plications, from chatbots and virtual assistants to content creation and language\n",
      "translation.translation.\n",
      "One of the key strengths of large language models is their ability to perform\n",
      "zero-shot and few-shot learning. This means that they can generalize from asmall number of examples or even perform tasks they haven’t explicitly been\n",
      "trained on. For instance, when given a prompt to write a poem or summarizean article, these models can produce outputs that are remarkably sophisticated\n",
      "and human-like. This capability stems from their extensive training on diversedatasets, which enables them to recognize patterns and structures in language\n",
      "and apply this knowledge to new and unseen tasks.and apply this knowledge to new and unseen tasks.\n",
      "Despite their impressive capabilities, large language models also come with sig-nificant challenges and limitations. One major concern is their tendency to gen-\n",
      "erate plausible but incorrect or nonsensical answers. This issue arises becauseLLMs lack true understanding or consciousness; they generate responses based\n",
      "on statistical patterns rather than genuine comprehension. Additionally, theycan sometimes produce biased or harmful content, reflecting the biases present\n",
      "in the training data. Addressing these challenges requires ongoing research intoimproving model accuracy, safety, and fairness.\n",
      "The development and deployment of large language models have also raised\n",
      "important ethical and societal questions. For example, the potential misuse ofLLMs for generating fake news, deepfakes, or malicious content is a significant\n",
      "concern. Moreover, the computational resources required to train and run thesemodels are substantial, raising questions about their environmental impact and\n",
      "the equitable distribution of AI technology. Researchers and policymakers areactively working to establish guidelines and best practices to ensure that LLMs\n",
      "are used responsibly and ethically.\n",
      "Looking forward, the future of large language models is promising, with ongoingadvancements aimed at making them more eﬀicient, interpretable, and aligned\n",
      "with human values.\n",
      "Innovations such as fine-tuning, reinforcement learningfrom human feedback, and more eﬀicient architectures are being explored to\n",
      "enhance their performance and mitigate risks.\n",
      "As these models continue toAs these models continue to\n",
      "evolve, they hold the potential to transform various industries, from healthcare\n",
      "and education to entertainment and beyond, by enabling more natural andeffective human-computer interactions.\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "text2 = file_preprocessing(\"C:/Users/Admin/Documents/gen_ai_training/pdfs/LLMs.pdf\")\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarizer_pipeline(file_path):\n",
    "    pipe_summarize = pipeline(\n",
    "        \"summarization\",\n",
    "        model = base_model,\n",
    "        tokenizer = tokenizer\n",
    "    )\n",
    "    input_text = file_preprocessing(file_path)\n",
    "    result = pipe_summarize(input_text)\n",
    "    result = result[0]['summary_text']\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large language models have revolutionized NLP by generating human-like text using deep learning techniques. They can perform zero-shot and few-shot learning, generalize from a small number of examples, and produce coherent and contextually relevant responses. However, they also come with significant challenges and limitations, such as their tendency to gen-erate plausible but incorrect or nonsensical answers, and can sometimes produce biased or harmful content. Addressing these challenges requires ongoing research into improving model accuracy, safety, and fairness. The future of large language models is promising, with ongoing advancements aimed at making them more efficient, interpretable, and aligned with human values. Innovations such as fine-tuning, reinforcement learning from human feedback, and more efficient architectures are being explored to enhance their performance and mitigate risks. These models have the potential to transform various industries, from healthcare and education to entertainment and beyond.\n"
     ]
    }
   ],
   "source": [
    "summary = summarizer_pipeline(\"C:/Users/Admin/Documents/gen_ai_training/pdfs/LLMs.pdf\")\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
