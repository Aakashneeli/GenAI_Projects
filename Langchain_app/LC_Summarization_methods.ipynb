{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv , find_dotenv\n",
    "import os\n",
    "dotenv_path = find_dotenv()\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if api_key is None:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "import glob \n",
    "from langchain.llms import OpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import fitz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STUFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper \"Retrieval-Augmented Generation for Large Language Models: A Survey\" by Yunfan Gao et al. explores the integration of Retrieval-Augmented Generation (RAG) with Large Language Models (LLMs) to address challenges such as hallucination, outdated knowledge, and non-transparent reasoning. RAG enhances LLMs by incorporating external knowledge, improving accuracy and credibility, especially for knowledge-intensive tasks. The survey categorizes RAG into three paradigms: Naive RAG, Advanced RAG, and Modular RAG, each progressively refining the retrieval, generation, and augmentation processes. It discusses the evolution of RAG, including indexing optimization, query transformation, and embedding techniques. The paper also highlights the importance of iterative, recursive, and adaptive retrieval methods and evaluates RAG's performance across various tasks using benchmarks and tools. Future research directions include improving robustness, integrating RAG with fine-tuning, and exploring multi-modal applications. The survey underscores RAG's potential in enhancing LLM capabilities and its growing ecosystem in AI applications.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#StuffDocumentChain\n",
    "#The chain will take a list of documents, insert them all into a prompt, and pass that prompt to an LLM\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "#Defining Prompt \n",
    "prompt_template = \"\"\"Write a concise summary of the following:\n",
    "\"{text}\"\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "#defining LLM\n",
    "llm = ChatOpenAI(temperature=0.2, model_name=\"gpt-4o\")\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "loader  = PyPDFLoader(\"C:\\\\Users\\\\Admin\\\\Documents\\\\gen_ai_training\\\\pdfs\\\\rag.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "stuff_chain = load_summarize_chain(llm, chain_type=\"stuff\", prompt = prompt)\n",
    "\n",
    "print(stuff_chain.invoke(docs)[\"output_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAP-REDUCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper discusses the challenges faced by Large Language Models (LLMs) and introduces Retrieval-Augmented Generation (RAG) as a solution to enhance accuracy and credibility by incorporating knowledge from external databases. It reviews the progression of RAG paradigms, highlights technologies embedded in retrieval, generation, and augmentation techniques, and introduces an evaluation framework and benchmark. The research on RAG focuses on leveraging LLMs for pre-training, fine-tuning, and inference stages, categorizing RAG into Naive, Advanced, and Modular stages. The RAG process for question answering involves indexing, retrieval, and generation, with challenges and improvements discussed. Various methods and optimization strategies for RAG models, including query transformation, query routing, and embedding, are explored. The RAG system includes iterative, recursive, and adaptive retrieval processes to improve information retrieval efficiency and relevance. The Self-RAG model uses reflection tokens to activate relevant information retrieval. Evaluation metrics and downstream tasks for RAG models are discussed, along with challenges and future research directions. The RAG ecosystem is evolving towards specialization and customization for production environments, expanding beyond text-based question-answering to include multimodal data. The integration of RAG with other AI methodologies is enhancing its capabilities, with opportunities for further research to improve robustness and handling of extended contexts. The growing ecosystem of RAG in AI applications emphasizes the development of supportive tools and refined evaluation methodologies. Various research papers and resources related to language models, question answering, knowledge graphs, and text embeddings are provided, covering a range of topics and techniques to enhance the performance and capabilities of large language models through retrieval augmentation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "loader  = PyPDFLoader(\"C:\\\\Users\\\\Admin\\\\Documents\\\\gen_ai_training\\\\pdfs\\\\rag.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "#Defining Prompt \n",
    "map_template = \"\"\" \n",
    "The following is a set of documents\n",
    "{docs}\n",
    "Based on this list of docs, please identify the main themes \n",
    "Helpful Answer:\n",
    "\"\"\"\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "\n",
    "combine_template = \"\"\"Here are the main themes identified from the set of documents:\n",
    "{themes}\n",
    "\n",
    "Please provide a consolidated summary of these themes.\n",
    "Helpful Answer:\n",
    "\"\"\"\n",
    "\n",
    "combine_prompt = PromptTemplate.from_template(combine_template)\n",
    "\n",
    "\n",
    "\n",
    "#defining LLMx``\n",
    "llm2 = ChatOpenAI(temperature=0.2, model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "map_reduce_chain = load_summarize_chain(llm2, chain_type= \"map_reduce\",  return_intermediate_steps=True)\n",
    "\n",
    "print(map_reduce_chain.invoke(docs)[\"output_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REFINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- A. Yang et al. presented a paper on \"Vid2seq\" which focuses on pretraining a visual language model for dense video captioning at the IEEE/CVF Conference on Computer Vision and Pattern Recognition in 2023.\n",
      "- The paper by N. Nashid et al. discusses retrieval-based prompt selection for code-related few-shot learning at the 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE).\n"
     ]
    }
   ],
   "source": [
    "question_prompt_template = \"\"\"\n",
    "        Please provide a summary of the following text.\n",
    "        TEXT: {text}\n",
    "        SUMMARY:                          \n",
    "\"\"\"\n",
    "\n",
    "question_prompt = PromptTemplate.from_template(question_prompt_template)\n",
    "\n",
    "refine_prompt_template = \"\"\"\n",
    "              Write a concise summary of the following text delimited by triple backquotes.\n",
    "              Return your response in bullet points which covers the key points of the text.\n",
    "              ```{text}```\n",
    "              BULLET POINT SUMMARY:\n",
    "              \"\"\"\n",
    "\n",
    "refine_prompt = PromptTemplate.from_template(refine_prompt_template)\n",
    "\n",
    "\n",
    "refine_chain = load_summarize_chain(llm2, chain_type = \"refine\",  question_prompt=question_prompt,refine_prompt=refine_prompt,return_intermediate_steps=True,)\n",
    "\n",
    "print(refine_chain.invoke(docs)[\"output_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
